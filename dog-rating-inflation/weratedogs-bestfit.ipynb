{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callysto's Weekly Data Visualization\n",
    "## WeRateDogs inflationary scoring\n",
    "\n",
    "\n",
    "### Reccommended grade level: 9-12\n",
    "\n",
    "### Instructions\n",
    "#### Step 1 (your only step): “Run” the cells to see the graphs\n",
    "Click “Cell” and select “Run All.” This will import the data and run all the code to make this week's data visualizations (scroll to the top after you’ve run the cells). **You don’t need to do any coding**.\n",
    "\n",
    "![instructions](https://github.com/callysto/data-viz-of-the-week/blob/main/images/instructions.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About This Notebook\n",
    "\n",
    "Callysto's Weekly Data Visualization is a learning resource that helps Grades 5-12 teachers and students grow and develop data literacy skills. We do this by providing a data visualization, like a graph, and asking teachers and students to interpret it. This companion resource walks learners through how the data visualization is created and interpreted using the data science process. The steps of this process are listed below and applied to each weekly topic.\n",
    "\n",
    "1. Question - What are we trying to answer?\n",
    "2. Gather - Find the data source(s) you will need.\n",
    "3. Organize - Arrange the data so that you can easily explore it.\n",
    "4. Explore - Examine the data to look for evidence to answer our question. This includes creating visualizations.\n",
    "5. Interpret - Explain how the evidence answers our question.\n",
    "6. Communicate - Reflect on the interpretation.\n",
    "\n",
    "### Acknowledgment\n",
    "\n",
    "This project and it's idea are based of [this](http://dhmontgomery.com/2017/03/dogrates/) data science exploration using, with permission, a data set of tweets collected by [Greg Baker](https://www.sfu.ca/computing/people/faculty/gregbaker.html), a senior lecturer at [SFU](https://www.sfu.ca) for use in his Computational Data Science class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question\n",
    "<div>\n",
    "<img src=\"./images/brent.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WeRateDogs](https://twitter.com/dog_rates) is a [popular](https://en.wikipedia.org/wiki/WeRateDogs) twitter account that offers humourous dog ratings and has spawned many memes. The twitter exchange above was a popular meme a few years ago, and the account is even [credited](https://www.npr.org/sections/alltechconsidered/2017/04/23/524514526/dogs-are-doggos-an-internet-language-built-around-love-for-the-puppers) with creating or formalizing the \"pupper\" and \"doggo\" lingo used to describe dogs over the internet. Much like 4chan users are credited with creating a 'LOLCAT' pigeon language in the earlier 2000s.\n",
    "\n",
    "Outside of exploring the history of internet meme's the account can demonstrate the concept of grade inflation. Usually when grade inflation is discussed, it is discussed in context of highschools or university giving out more high B and A grades leading to a dimishing value of those higher grades. Here we will see how scores given out by the WeRateDogs account may or may not be suffering from grade inflation. Hence, our question for this notebook is:\n",
    "* Are the WeRateDogs scores suffering from inflation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gather\n",
    "\n",
    "We will import the python libraries we need then read in our already collected dataset of WeRateDogs tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the dataset\n",
    "path = os.path.join('datasets', 'dog_rates_tweets.csv')\n",
    "data = pd.read_csv(path, dtype ={'text':str},parse_dates = ['created_at']).set_index(keys='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', data.shape[0], 'tweets in the dataset.\\nThe first few rows look like this:')\n",
    "#view the unmutated data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Organize\n",
    "This step will take some work.  Our data does not have any scores clearly visible. First we will need to extract scores from our tweets in an automated fashion. It would take far too long and be far too error prone to manually check eight-thousand tweets. Then we will have to ensure our manual proccess was robust enough for our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_rating(text):\n",
    "    \"\"\"a helper function to find any 'x/10'\n",
    "    the function only returns the first such score found in a tweet\"\"\"\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)/10',str(text))\n",
    "    if match:\n",
    "        top, btm = match[0].split('/')\n",
    "        return float(top)/float(btm)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the ratings\n",
    "data['rating']= data['text'].map(text_to_rating)\n",
    "#drop the actual content of the tweets\n",
    "data =data.drop(['text'], axis =1)\n",
    "#drop the 0/10 scores found\n",
    "data = data[np.isfinite(data['rating'])]\n",
    "#rename create_at\n",
    "data.rename(columns={'created_at':'date created'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='rating', ascending = False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data has a large range of values and the top few seems particularily large for an out of ten system. Very large datapoints are usually refered to as outliers, depending on the use, the data and the question outliers sometimes need to be removed, but other times are very key to painting an accurate picture of the data. Below, we do remove a number of the largest data points. This was decide by looking at the tweets and seeing if they were valid scores or somthing else. The scores about 14/10 all seemed irrelevant.\n",
    "\n",
    "Sometimes they were dates:\n",
    "![holiday score](./images/1776.png)\n",
    "\n",
    "\n",
    "Sometimes they were scores scrapped from conversations about scoring:\n",
    "![](./images/meta.png)\n",
    "\n",
    "\n",
    "Sometimes they were honourary:\n",
    "![](./images/honourary.png)\n",
    "\n",
    "This dataset did not contain any genuine scores about 14. However, the web scraping used to collect the data did not find every single tweet, so it is possibly missing valid 15/10 scores for espesially good dogs.\n",
    "\n",
    "the low scores, except for the 0/10, seemed to be genuine ratings. Many low scores, however, were rating animals other than dogs on the same scale. For our purposes we will consider those to be true ratings.  \n",
    "\n",
    "Many low rates given were like this one given to a goat:\n",
    "![](./images/goat.png)\n",
    "\n",
    "Since this data exploration isn't a serious one we can decide farily freely which datapoints to keeo and which to drop. However, in hard science or research data should never be removed without careful thought. Changing key points of data can grosly change the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove largest scores points\n",
    "data_cleaned = data.iloc[13:-3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(data):\n",
    "    \"\"\"A simple function that will take in a dataframe formated like ours \n",
    "    and produce a scatterplot with best fit line\n",
    "    input:\n",
    "        a pandas dataframe with 'rateing' and 'date created columns\n",
    "    return:\n",
    "        a plotly express scatterplot with best fit line(untitled)\n",
    "    \"\"\"\n",
    "    fig = px.scatter(data, x='date created', y='rating', trendline='ols')\n",
    "    #highlight the best fit line in red to make it more visible\n",
    "    fig.data[1].update(line_color='red')\n",
    "    #show the tweets in the legend\n",
    "    fig['data'][0]['showlegend']=True\n",
    "    fig['data'][0]['name']='Tweet'\n",
    "    # show the best fit line in the legend\n",
    "    fig['data'][1]['showlegend']=True\n",
    "    fig['data'][1]['name']='Best Fit Line (OLS)'\n",
    "    fig.update_layout(showlegend=True)\n",
    "    #show the plot\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_plot(data)\n",
    "fig.update_layout(title = 'Plot without outliers removed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the outliers are not removed the visual is very hard to make out any detail and the line best fitting the data has a slope of $2.8 \\times 10^{-9}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_plot(data_cleaned)\n",
    "fig.update_layout(title = 'WeRateDogs Scores Given Versus Time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the outliers are removed the visual clearly shows seperate scores and the slope is now $3.4\\times 10^{-9}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpret\n",
    "The positive slope on the red line reveals that scores have been increasing as time has gone on. This line is the line through the datapoints that minimizes the squared y-value distance between the line and datapoints. This method is called *Ordinary Least Squares* and is a common method for approximating a a straight line through a dataset to reveal a relationship.\n",
    "\n",
    "We can conclude that grade inflation has been affecting the WeRateDogs scores.\n",
    "\n",
    "However, removing the outliers did change the amount of that slope. Do you think they were fairly removed? Why or why not?\n",
    "\n",
    "Also this data is missing tweets from mid 2018. Do you think they'd change the best fit line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Communicate\n",
    "Below we will reflect on the new information that is presented from the data. When we look at the evidence, think about what you perceive about the information. Is this perception based on what the evidence shows? If others were to view it, what perceptions might they have? These writing prompts can help you reflect.\n",
    "\n",
    "* I used to think __ but now I know __.\n",
    "* I wish I knew more about __.\n",
    "* This visualization reminds me of __.\n",
    "* I really like __.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
